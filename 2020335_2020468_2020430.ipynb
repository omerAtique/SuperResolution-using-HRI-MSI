{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e47d7b4-cede-49f0-b4f0-e805c35112c3",
   "metadata": {},
   "source": [
    "# CS-351\n",
    "# SUPER RESOLUTION PROJECT\n",
    "\n",
    "1) Muhammad Omer Bin Atique 2020335\n",
    "2) Syed Ammar Bin Farrukh 2020468\n",
    "3) Sameer Arif Khan 2020430"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3355a-0da6-49cd-943d-3e901d39c8a4",
   "metadata": {},
   "source": [
    "## Import All Libraries For Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9656430d-b3fb-494a-b687-f39f6894a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 11:06:47.120468: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 11:06:47.644224: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-20 11:06:47.714861: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:06:47.714915: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-20 11:06:49.629216: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:06:49.629439: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:06:49.629458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random as rand\n",
    "import seaborn as sns\n",
    "import scipy as sci\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import cv2\n",
    "from math import log10, sqrt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "import mtcnn\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d48d9-ac3e-410f-b669-74474f34a7f5",
   "metadata": {},
   "source": [
    "## Get Folder Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286030d5-1267-42d6-b28d-d18addf7821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['balloons_ms', 'beads_ms', 'cd_ms', 'chart_and_stuffed_toy_ms', 'clay_ms', 'cloth_ms', 'egyptian_statue_ms', 'face_ms', 'fake_and_real_beers_ms', 'fake_and_real_food_ms', 'fake_and_real_lemon_slices_ms', 'fake_and_real_lemons_ms', 'fake_and_real_peppers_ms', 'fake_and_real_strawberries_ms', 'fake_and_real_sushi_ms', 'fake_and_real_tomatoes_ms', 'feathers_ms', 'flowers_ms', 'glass_tiles_ms', 'hairs_ms', 'jelly_beans_ms', 'oil_painting_ms', 'paints_ms', 'photo_and_face_ms', 'pompoms_ms', 'real_and_fake_apples_ms', 'real_and_fake_peppers_ms', 'sponges_ms', 'stuffed_toys_ms', 'superballs_ms', 'thread_spools_ms', 'watercolors_ms']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./dataset/\"\n",
    "groundtruth_images = []\n",
    "image_size = 512\n",
    "patch_size = 64\n",
    "\n",
    "subfolders = sorted([f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))])\n",
    "\n",
    "print(subfolders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aadaa8-15ea-4433-8c55-ccdab80de268",
   "metadata": {},
   "source": [
    "## Import All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c08df5e-599e-4f5a-8c2a-bf10a3d8e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/balloons_ms\n",
      "./dataset/beads_ms\n",
      "./dataset/cd_ms\n",
      "./dataset/chart_and_stuffed_toy_ms\n",
      "./dataset/clay_ms\n",
      "./dataset/cloth_ms\n",
      "./dataset/egyptian_statue_ms\n",
      "./dataset/face_ms\n",
      "./dataset/fake_and_real_beers_ms\n",
      "./dataset/fake_and_real_food_ms\n",
      "./dataset/fake_and_real_lemon_slices_ms\n",
      "./dataset/fake_and_real_lemons_ms\n",
      "./dataset/fake_and_real_peppers_ms\n",
      "./dataset/fake_and_real_strawberries_ms\n",
      "./dataset/fake_and_real_sushi_ms\n",
      "./dataset/fake_and_real_tomatoes_ms\n",
      "./dataset/feathers_ms\n",
      "./dataset/flowers_ms\n",
      "./dataset/glass_tiles_ms\n",
      "./dataset/hairs_ms\n",
      "./dataset/jelly_beans_ms\n",
      "./dataset/oil_painting_ms\n",
      "./dataset/paints_ms\n",
      "./dataset/photo_and_face_ms\n",
      "./dataset/pompoms_ms\n",
      "./dataset/real_and_fake_apples_ms\n",
      "./dataset/real_and_fake_peppers_ms\n",
      "./dataset/sponges_ms\n",
      "./dataset/stuffed_toys_ms\n",
      "./dataset/superballs_ms\n",
      "./dataset/thread_spools_ms\n",
      "./dataset/watercolors_ms\n",
      "Ground Truth Hyperspectral Images Shape: (7200, 64, 64, 31)\n"
     ]
    }
   ],
   "source": [
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(dataset_path, subfolder)\n",
    "    print(subfolder_path)\n",
    "\n",
    "    hyperspectral_image = np.empty((image_size, image_size, 31), dtype=np.uint8)\n",
    "    i = 0\n",
    "    for filename in sorted(os.listdir(subfolder_path)):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_file = filename\n",
    "            image_path = os.path.join(subfolder_path, image_file)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            hyperspectral_image[:, :, i] = image\n",
    "            i = i + 1\n",
    " \n",
    "    for y in range(0, image_size - patch_size + 1, stride):\n",
    "        for x in range(0, image_size - patch_size + 1, stride):\n",
    "            patch = hyperspectral_image[y:y+patch_size, x:x+patch_size, :]\n",
    "            groundtruth_images.append(patch)\n",
    "\n",
    "groundtruth_arrays = np.array(groundtruth_images)\n",
    "\n",
    "print(\"Ground Truth Hyperspectral Images Shape:\", groundtruth_arrays.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96316a3-2e75-4454-a4a0-e46c6d73a053",
   "metadata": {},
   "source": [
    "## Obtain A Low Resolution HSI Image of Shape (8,8,31)\n",
    "One will simulate an image with low spatial dimensions and high spectral dimension. You'll obtain this by applying a 8,8 averaging filter on each band of your image of resolution 64,64,31 to obtain an image of resolution of 8,8,31. Let's call this one a low(spatial) resolution HSIimage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e785c88a-e329-4f92-b210-37755682a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import uniform_filter\n",
    "\n",
    "filter_size = (8, 8)\n",
    "\n",
    "lowres_hsi_images = np.zeros((groundtruth_arrays.shape[0], 8, 8, 31))\n",
    "for i in range(groundtruth_arrays.shape[0]):\n",
    "    for j in range(groundtruth_arrays.shape[3]):\n",
    "        band = groundtruth_arrays[i, :, :, j]\n",
    "        lowres_band = uniform_filter(band, size=filter_size)\n",
    "        lowres_hsi_images[i, :, :, j] = lowres_band[::8, ::8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034cb32d-31b4-483b-905e-ba6073bcf6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-Resolution HSI Images Shape: (7200, 8, 8, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"Low-Resolution HSI Images Shape:\", lowres_hsi_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6262054c-4b6d-49c2-8af0-7d9d9136b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "highres_rgb_images = np.zeros((groundtruth_arrays.shape[0], 64, 64, 3))\n",
    "for i in range(groundtruth_arrays.shape[0]):\n",
    "    for j in range(3):\n",
    "        start_band = j * 10\n",
    "        end_band = (j + 1) * 10\n",
    "        rgb_band_avg = np.mean(groundtruth_arrays[i, :, :, start_band:end_band], axis=-1)\n",
    "        highres_rgb_images[i, :, :, j] = rgb_band_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d89cc63-f337-469c-b451-323e90c1307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Resolution RGB Images Shape: (7200, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"High-Resolution RGB Images Shape:\", highres_rgb_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f62ed5-88da-4026-b25b-1128625cf624",
   "metadata": {},
   "source": [
    "## Split Images To Form A Test And Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f428ef-3096-4a03-9902-83d5346d7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "num_samples = groundtruth_arrays.shape[0]\n",
    "num_train_samples = int(train_ratio * num_samples)\n",
    "train_lowres_hsi = lowres_hsi_images[:num_train_samples]\n",
    "train_highres_rgb = highres_rgb_images[:num_train_samples]\n",
    "test_lowres_hsi = lowres_hsi_images[num_train_samples:]\n",
    "test_highres_rgb = highres_rgb_images[num_train_samples:]\n",
    "y_train = groundtruth_arrays[:num_train_samples]\n",
    "y_test = groundtruth_arrays[num_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_lowres_hsi = train_lowres_hsi / 255.0\n",
    "train_highres_rgb = train_highres_rgb / 255.0\n",
    "test_lowres_hsi = test_lowres_hsi / 255.0\n",
    "test_highres_rgb = test_highres_rgb / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c01bb266-a531-4a7c-8811-ffd831d5c500",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 11:08:40.143360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-20 11:08:40.144552: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:08:40.144854: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:08:40.145033: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:08:40.145177: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:08:40.145292: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:08:40.145405: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:08:40.145518: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:08:40.145632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/omer_atique/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-20 11:08:40.145653: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-20 11:08:40.146978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 8, 8, 31)]   0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 64, 64, 31)  61535       ['input_2[0][0]']                \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64, 64, 34)   0           ['input_1[0][0]',                \n",
      "                                                                  'conv2d_transpose[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 64, 64, 64)   19648       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 64, 64, 128)  73856       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64, 64, 128)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 256)  295168      ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 64, 64, 256)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 128)  295040     ['leaky_re_lu_2[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 64, 64, 128)  0           ['conv2d_transpose_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 64, 64, 64)  73792       ['leaky_re_lu_3[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 64, 64, 64)   0           ['conv2d_transpose_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 64, 64, 31)   17887       ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 836,926\n",
      "Trainable params: 836,926\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Concatenate, Conv2D, Conv2DTranspose, BatchNormalization, Activation, LeakyReLU\n",
    "from keras.models import Model\n",
    "\n",
    "def create_super_resolution_model(input_shape_hr, input_shape_lr, output_shape):\n",
    "    hr_input = Input(shape=input_shape_hr)\n",
    "    \n",
    "    lr_input = Input(shape=input_shape_lr)\n",
    "\n",
    "    upsampled_lr = Conv2DTranspose(31, (8, 8), strides=(8, 8), padding='same')(lr_input)\n",
    "\n",
    "    concat = Concatenate()([hr_input, upsampled_lr])\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=1, padding='same')(concat)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, (3, 3), strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(256, (3, 3), strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    \n",
    "    x = Conv2DTranspose(128, (3, 3), strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    output = Conv2D(31, (3, 3), strides=1, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[hr_input, lr_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "input_shape_hr = (64, 64, 3)\n",
    "input_shape_lr = (8, 8, 31)\n",
    "output_shape = (64, 64, 31)\n",
    "\n",
    "model = create_super_resolution_model(input_shape_hr, input_shape_lr, output_shape)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846fe37-f4bb-4ac4-8754-c899ec56b1cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 982s 5s/step - loss: 0.0084 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 931s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 931s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 1291s 7s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      " 20/180 [==>...........................] - ETA: 12:52 - loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse') \n",
    "model.fit([train_highres_rgb, train_lowres_hsi], y_train, validation_data=([test_highres_rgb, test_lowres_hsi], y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29899a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
